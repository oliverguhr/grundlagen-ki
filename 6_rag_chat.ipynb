{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "deployment = os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = endpoint, \n",
    "  api_key= api_key,  \n",
    "  api_version=\"2024-08-01-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_text = [\n",
    "    {\"role\":\"system\",\"content\":\"Du bist ein AI Assistent der Menschen hilft Antworten zu finden.\"},\n",
    "    {\"role\":\"user\",\"content\":\"Was ist die Telekom MMS?\"},]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=deployment,\n",
    "  messages = message_text,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) ist ein Modell, das aus zwei Hauptkomponenten besteht: einem Retrieval-System und einem Generierungsmodell. Das Ziel von RAG ist es, die Qualität der Textgenerierung zu verbessern, indem es relevante und aktuelle Informationen aus einer großen Datenbank oder einem Datensatz abruft und diese Informationen zur Unterstützung des Generierungsprozesses verwendet.\n",
    "\n",
    "Das Verfahren funktioniert in zwei Schritten:\n",
    "\n",
    "1. Retrieval (Abruf): Zuerst sucht das Retrieval-System in einer umfangreichen Datenquelle nach Informationen, die relevant für die aktuelle Anfrage oder den Kontext sind. Diese Informationen werden als Unterstützung für die Textgenerierung ausgewählt.\n",
    "2. Generation (Generierung): Anschließend verwendet das Generierungsmodell, in der Regel ein LLM, sowohl den ursprünglichen Text als auch die abgerufenen Informationen, um eine präzise und kontextuell relevante Antwort zu erzeugen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Beantworten die Fragen der Nutzenden auf der Grundlage des unten stehenden Kontexts.\n",
    "\n",
    "context:\n",
    "Die Deutsche Telekom MMS GmbH (Telekom MMS) ist ein Digital-Dienstleister, der sich als Begleiter von Großkonzernen und mittelständischen Unternehmen bei der digitalen Transformation versteht.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "message_text = [\n",
    "    {\"role\":\"system\",\"content\":prompt},\n",
    "    {\"role\":\"user\",\"content\":\"Was ist die Telekom MMS?\"},]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=deployment,\n",
    "  messages = message_text,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden und durchsuchen von eigenen Daten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader('./docs', glob=\"**/*.md\")\n",
    "data = loader.load()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# das array enthält alle texte aller Dateien\n",
    "\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wie können wir die Texte in kürzere Abschnitte unterteilen?\n",
    "\n",
    "Web Demo [Chunk Visualizer](https://huggingface.co/spaces/m-ric/chunk_visualizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter,SentenceTransformersTokenTextSplitter,MarkdownHeaderTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=450, chunk_overlap=50)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "print(len(all_splits))\n",
    "all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# setup Chroma in-memory, for easy prototyping. Can add persistence easily!\n",
    "chroma = chromadb.Client()\n",
    "\n",
    "# Create collection. get_collection, get_or_create_collection, delete_collection also available!\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"ibm-granite/granite-embedding-278m-multilingual\")\n",
    "try:\n",
    "    chroma.delete_collection(\"documents\")\n",
    "except:\n",
    "    pass\n",
    "collection = chroma.get_or_create_collection(\"documents\",embedding_function=sentence_transformer_ef)\n",
    "\n",
    "# Add docs to the collection. Can also update and delete. Row-based API coming soon!\n",
    "collection.add(\n",
    "    documents=[item.page_content for item in all_splits], # we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well\n",
    "    metadatas=[item.metadata for item in all_splits], # filter on these!\n",
    "    ids=[str(id) for id in range(0,len(all_splits))], # unique for each doc\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query/search 2 most similar results. You can also .get by id\n",
    "results = collection.query(\n",
    "    query_texts=[\"Wer ist der Schulungsleiter\"],\n",
    "    n_results=2,\n",
    "    # where={\"metadata_field\": \"is_equal_to_this\"}, # optional filter\n",
    "    #where_document={\"$contains\":\"Oliver\"}  # optional filter\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rag_chat(user_message, history):\n",
    "    # 1. Abruf relevanter Dokumente (Retrieval)\n",
    "    # Wir suchen in unserer Collection nach den 2 ähnlichsten Dokumenten\n",
    "    results = collection.query(\n",
    "        query_texts=[user_message],\n",
    "        n_results=2\n",
    "    )\n",
    "\n",
    "    # 2. Aufbereitung des Kontexts\n",
    "    # Die gefundenen Texte und Quellen werden lesbar zusammengestellt\n",
    "    context = \"\"\n",
    "    for doc, metadata in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        context += f\"Quelle: {metadata['source']}\\n\"\n",
    "        context += f\"{doc}\\n\"\n",
    "    \n",
    "    # 3. Erstellen des Prompts (Generation)\n",
    "    # Wir geben dem LLM die Anweisung, nur mit dem gefundenen Wissen zu antworten\n",
    "    system_prompt = f\"\"\"Beantworte die Fragen der Nutzenden auf der Grundlage des Kontexts. Gib die Quelle der Informationen an.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Wenn die Informationen zur Frage nicht im Kontext enthalten sind, dann antworte \"Ich weiß es nicht.\"\n",
    "    \"\"\"\n",
    "\n",
    "    # 4. Anfrage an das KI-Modell senden\n",
    "    message_text = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=message_text\n",
    "    )\n",
    "\n",
    "    # Die Antwort des Modells zurückgeben\n",
    "    return completion.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chat(\"Wer ist der Schulungsleiter?\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "# Erstellen der Benutzeroberfläche\n",
    "# ChatInterface erstellt automatisch ein Chat-Fenster\n",
    "demo = gr.ChatInterface(\n",
    "    fn=rag_chat, \n",
    "    title=\"RAG Chatbot\", \n",
    "    description=\"Ein einfacher Bot, der Fragen zu den geladenen Dokumenten beantwortet.\"\n",
    ")\n",
    "\n",
    "# Starten der Anwendung \n",
    "# (share=True erstellt öffentlichen Link, hier lassen wir es lokal)\n",
    "demo.launch(server_port=7878)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nächster Schritt\n",
    "\n",
    "Was passiert, wenn der Nutzer \"Erzähle mir mehr!\" im Chat eingibt? Wie können wir diese Feature implementieren?\n",
    "\n",
    "Bonus: \n",
    "\n",
    "* Erstelle einen ChatBot für den EU AI Act. Welche Schritte sind dafür notwendig?\n",
    "\n",
    "* Einstieg in [LangChain](https://www.langchain.com/). Die drei Notebooks im LangChain Ordner demonstrien wie man diesen Workflow mit Langchain implementiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grundlagen-ki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
